<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2 | MBofillRoig</title>
    <link>https://academic-demo.netlify.app/publication-type/2/</link>
      <atom:link href="https://academic-demo.netlify.app/publication-type/2/index.xml" rel="self" type="application/rss+xml" />
    <description>2</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 04 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://academic-demo.netlify.app/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>2</title>
      <link>https://academic-demo.netlify.app/publication-type/2/</link>
    </image>
    
    <item>
      <title>Treatment-control comparisons in platform trials including non-concurrent controls</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bp_2025/</link>
      <pubDate>Fri, 04 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bp_2025/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; 
Shared controls in platform trials comprise concurrent and non-concurrent controls. For a given experimental arm, non-concurrent controls refer to data from patients allocated to the control arm before the arm enters the trial. The use of non-concurrent controls in the analysis is attractive because it may increase the trial&amp;rsquo;s power of testing treatment differences while decreasing the sample size. However, since arms are added sequentially in the trial, randomization occurs at different times, which can introduce bias in the estimates due to time trends. In this article, we present methods to incorporate non-concurrent control data in treatment-control comparisons, allowing for time trends. We focus mainly on frequentist approaches that model the time trend and Bayesian strategies that limit the borrowing level depending on the heterogeneity between concurrent and non-concurrent controls. We examine the impact of time trends, overlap between experimental treatment arms and entry times of arms in the trial on the operating characteristics of treatment effect estimators for each method under different patterns for the time trends. We argue under which conditions the methods lead to type 1 error control and discuss the gain in power compared to trials only using concurrent controls by means of a simulation study in which methods are compared.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Design of Trials with Composite Endpoints with the R Package CompAREdesign</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/cbg_2022/</link>
      <pubDate>Sat, 10 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/cbg_2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract&lt;/strong&gt; 
Composite endpoints are widely used as primary endpoints in clinical trials. Designing trials with time-to-event endpoints can be particularly challenging because the proportional hazard assumption usually does not hold when using a composite endpoint, even when the premise remains true for their components. Consequently, the conventional formulae for sample size calculation do not longer apply. We present the R package CompAREdesign by means of which the key elements of trial designs, such as the sample size and effect sizes, can be computed based on the information on the composite endpoint components. CompAREdesign provides the functions to 
assess the sensitivity and robustness of design calculations to variations in initial values and assumptions. Furthermore, we describe other features of the package, such as functions for the design of trials with binary composite endpoints, and functions to simulate trials with composite endpoints under a wide range of scenarios.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Online Closure Principle</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/fbb_2022/</link>
      <pubDate>Fri, 22 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/fbb_2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; The closure principle is fundamental in multiple testing and has been used to derive many efficient procedures with familywise error rate control. However, it is often not suitable for modern research, as more flexible multiple testing settings are considered where not all hypotheses are known at the beginning of the evaluation. In this paper, we focus on online multiple testing where a possibly infinite sequence of hypotheses is tested over time. At each step, it must be decided on the current hypothesis without having any information about the hypotheses that have not been tested yet. Our main contribution is a new online closure principle which ensures that the resulting closed procedure can be applied in the online setting. We prove that any familywise error rate (FWER) controlling online procedure can be derived by this online closure principle. In addition, we demonstrate how short-cuts of these online closed procedures can be obtained under a suitable consonance property and apply the results in order to construct new online multiple testing methods. Finally, the new online closure principle is used to derive an improvement of the currently most promising online procedure with FWER control, the ADDIS-Spending under local dependence.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Statistical modeling to adjust for time trends in adaptive platform trials utilizing non-concurrent controls</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/pk_2024/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/pk_2024/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An exhaustive ADDIS principle for online FWER control</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/fbb_2023b/</link>
      <pubDate>Wed, 28 Feb 2024 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/fbb_2023b/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; In this paper we consider online multiple testing with familywise error rate (FWER) control, where the probability of committing at least one type I error shall remain under control while testing a possibly infinite sequence of hypotheses over time. Currently, Adaptive-Discard (ADDIS) procedures seem to be the most promising online procedures with FWER control in terms of power. Now, our main contribution is a uniform improvement of the ADDIS principle and thus of all ADDIS procedures. This means, the methods we propose reject as least as much hypotheses as ADDIS procedures and in some cases even more, while maintaining FWER control. In addition, we show that our principle cannot be further uniformly improved. Finally, we apply the new principle to derive uniform improvements of the ADDIS-Spending and ADDIS-Graph.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimal allocation strategies in platform trials with continuous endpoints</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bgmp_2023/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bgmp_2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NCC: An R-package for analysis and simulation of platform trials with non-concurrent controls</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/kb_2023/</link>
      <pubDate>Wed, 01 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/kb_2023/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Platform trials evaluate the efficacy of multiple treatments, allowing for late entry of the experimental arms and enabling efficiency gains by sharing controls. The power of individual treatment-control comparisons in such trials can be improved by utilizing non-concurrent controls in the analysis. We present the R-package NCC for the design and analysis of platform trials using non-concurrent controls. NCC allows for simulating platform trials and evaluating the properties of analysis methods that make use of non-concurrent controls in a variety of settings. We describe the main NCC functions and show how to use the package to simulate and analyse platform trials by means of specific examples.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On the use of non-concurrent controls in platform trials: A scoping review</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bh_2022/</link>
      <pubDate>Thu, 17 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bh_2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Platform trials gained popularity during the last few years as they increase flexibility compared to multi-arm trials by allowing new experimental arms entering when the trial already started. Using a shared control group in platform trials increases the trial efficiency compared to separate trials.
Because of the later entry of some of the experimental treatment arms, the
shared control group includes concurrent and non-concurrent control data. For a
given experimental arm, non-concurrent controls refer to patients allocated to
the control arm before the arm enters the trial, while concurrent controls
refer to control patients that are randomised concurrently to the experimental
arm. Using non-concurrent controls can result in bias in the estimate in case
of time trends if the appropriate methodology is not used and the assumptions
are not met. In this paper, we faced two main objectives. In the first, we
aimed to identify the methods currently available for incorporating
non-concurrent controls, clarify the key concepts and assumptions, and name the
main characteristics of each method. For this purpose, we systematically
searched research articles on methods to include non-concurrent controls. The
second objective is to summarise the current regulatory view on non-concurrent
controls to clarify the key concepts and current guidance. Therefore, we
conducted a systematic search in regulatory guidelines regarding using
non-concurrent controls and summarised the most relevant arguments and
recommended methods. Finally, we discuss the advantages and potential caveats
of using non-concurrent controls.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive clinical trial designs with blinded selection of binary composite endpoints and sample size reassessment</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bgpk_2022/</link>
      <pubDate>Sun, 04 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bgpk_2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; For randomized clinical trials where a single, primary, binary endpoint would require unfeasibly large sample sizes, composite endpoints are widely chosen as the primary endpoint. Despite being commonly used, composite endpoints entail challenges in designing and interpreting results. Given that the components may be of different relevance and have different effect sizes, the choice of components must be made carefully. Especially, sample size calculations for composite binary endpoints depend not only on the anticipated effect sizes and event probabilities of the composite components, but also on the correlation between them. However, information on the correlation between endpoints is usually not reported in the literature which can be an obstacle for planning of future sound trial design. 
We consider two-arm randomized controlled trials with a primary composite binary endpoint and an endpoint that consists only of the clinically more important component of the composite endpoint. We propose a trial design that allows an adaptive modification of the primary endpoint based on blinded information obtained at an interim analysis. Especially, we consider a decision rule to select between a composite endpoint and its most relevant component as primary endpoint. The decision rule chooses the endpoint with the lower estimated required sample size. Additionally, the sample size is reassessed using the estimated event probabilities and correlation, and the expected effect sizes of the composite components. We investigate the statistical power and significance level under the proposed design through simulations. We show that the adaptive design is equally or more powerful than designs without adaptive modification on the primary endpoint. Besides, the targeted power is achieved even if the correlation is misspecified at the planning stage while maintaining the type 1 error. All the computations are implemented in R and illustrated by means of a peritoneal dialysis trial.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On model-based time trend adjustments in platform trials with non-concurrent controls</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bp_2021/</link>
      <pubDate>Thu, 04 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bp_2021/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Platform trials can evaluate the efficacy of several treatments compared to a control. The number of treatments is not fixed, as arms may be added or removed as the trial progresses. Platform trials are more efficient than independent parallel-group trials because of using shared control groups. For arms entering the trial later, not all patients in the control group are randomised concurrently. The control group is then divided into concurrent and non-concurrent controls. Using non-concurrent controls (NCC) can improve the trial&amp;rsquo;s efficiency, but can introduce bias due to time trends.
We focus on a platform trial with two treatment arms and a common control arm. Assuming that the second treatment arm is added later, we assess the robustness of model-based approaches to adjust for time trends when using NCC. We consider approaches where time trends are modeled as linear or as a step function, with steps at times where arms enter or leave the trial. For trials with continuous or binary outcomes, we investigate the type 1 error (t1e) rate and power of testing the efficacy of the newly added arm under a range of scenarios. In addition to scenarios where time trends are equal across arms, we investigate settings with trends that are different or not additive in the model scale.
A step function model fitted on data from all arms gives increased power while controlling the t1e, as long as the time trends are equal for the different arms and additive on the model scale. This holds even if the trend&amp;rsquo;s shape deviates from a step function if block randomisation is used. But if trends differ between arms or are not additive on the model scale, t1e control may be lost.
The efficiency gained by using step function models to incorporate NCC can outweigh potential biases. However, the specifics of the trial, plausibility of different time trends, and robustness of results should be considered.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Commentary: Two approaches to analyze platform trials incorporating non-concurrent controls with a common assumption</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bkmp_2022/</link>
      <pubDate>Sat, 04 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bkmp_2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A class of two-sample nonparametric statistics for binary and time-to-event outcomes</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bg_2020/</link>
      <pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bg_2020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; We propose a class of two-sample statistics for testing the equality of proportions and the equality of survival functions. We build our proposal on a weighted combination of a score test for the difference in proportions and a Weighted Kaplan-Meier statistic-based test for the difference of survival functions. The proposed statistics are fully non-parametric and do not rely on the proportional hazards assumption for the survival outcome. We present the asymptotic distribution of these statistics, propose a variance estimator and show their asymptotic properties under fixed and local alternatives. We discuss different choices of weights including those that control the relative relevance of each outcome and emphasize the type of difference to be detected in the survival outcome. We evaluate the performance of these statistics with a simulation study, and illustrate their use with a randomized phase III cancer vaccine trial. We have implemented the proposed statistics in the R package SurvBin, available on GitHub (&lt;a href=&#34;https://github.com/MartaBofillRoig/SurvBin%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/MartaBofillRoig/SurvBin)&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Design of phase III trials with long-term survival outcomes based on short-term binary results</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bsg_2020/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bsg_2020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; 
Pathologic complete response (pCR) is a common primary endpoint for a phase II trial or even accelerated approval of neoadjuvant cancer therapy. If granted, a two-arm confirmatory trial is often required to demonstrate the efficacy with a time-to-event outcome such as overall survival. However, the design of a subsequent phase III trial based on prior information on the pCR effect is not straightforward. Aiming at designing such phase III trials with overall survival as primary endpoint using pCR information from previous trials, we consider a mixture model that incorporates both the survival and the binary endpoints. We propose to base the comparison between arms on the difference of the restricted mean survival times, and show how the effect size and sample size for overall survival rely on the probability of the binary response and the survival distribution by response status, both for each treatment arm. Moreover, we provide the sample size calculation under different scenarios and accompany them with an R package where all the computations have been implemented. We evaluate our proposal with a simulation study, and illustrate its application through a neoadjuvant breast cancer trial.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A new approach for sizing trials with composite binary endpoints using anticipated marginal values and accounting for the correlation between components</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bg_2018/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bg_2018/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Composite binary endpoints are increasingly used as primary endpoints in clinical trials. When designing a trial, it is crucial to determine the appropriate sample size for testing the statistical differences between treatment groups for the primary endpoint. As shown in this work, when using a composite binary endpoint to size a trial, one needs to specify the event rates and the effect sizes of the composite components as well as the correlation between them. In practice, the marginal parameters of the components can be obtained from previous studies or pilot trials, however, the correlation is often not previously reported and thus usually unknown. We first show that the sample size for composite binary endpoints is strongly dependent on the correlation and, second, that slight deviations in the prior information on the marginal parameters may result in underpowered trials for achieving the study objectives at a pre-specified significance level. We propose a general strategy for calculating the required sample size when the correlation is not specified, and accounting for uncertainty in the marginal parameter values. We present the web platform CompARE to characterize composite endpoints and to calculate the sample size just as we propose in this paper. We evaluate the performance of the proposal with a simulation study, and illustrate it by means of a real case study using CompARE.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selection of composite binary endpoints in clinical trials</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bg_2017/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bg_2017/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; The choice of a primary endpoint is an important issue when designing a clinical trial. It is common to use composite endpoints as a primary endpoint because it increases the number of observed events, captures more information and is expected to increase the power. However, combining events that have no similar clinical importance and have different treatment effects makes the interpretation of the results cumbersome and might reduce the power of the corresponding tests. Gómez and Lagakos proposed the ARE (asymptotic relative efficiency) method to choose between a composite or one of its components as primary endpoint comparing the efficacy of a treatment based on the times to each of these endpoints. The aim of this paper is to expand the ARE method to binary endpoints. We show that the ARE method depends on six parameters including the degree of association between components, event proportion, and effect of therapy given by the corresponding odds ratio of the single endpoints. A case study is presented to illustrate the methodology. We conclude with efficient guidelines for discerning which could be the best suited primary endpoint given anticipated parameters.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
