<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Publications | MBofillRoig</title>
    <link>https://academic-demo.netlify.app/publication/</link>
      <atom:link href="https://academic-demo.netlify.app/publication/index.xml" rel="self" type="application/rss+xml" />
    <description>Publications</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 04 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://academic-demo.netlify.app/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>Publications</title>
      <link>https://academic-demo.netlify.app/publication/</link>
    </image>
    
    <item>
      <title>Adaptive clinical trial designs with selection of composite endpoints and sample size reassessment</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bgpk_2022/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bgpk_2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; For randomized clinical trials where a single, primary, binary endpoint would require unfeasibly large sample sizes, composite endpoints are widely chosen as the primary endpoint. Despite being commonly used, composite endpoints entail challenges in designing and interpreting results. Given that the components may be of different relevance and have different effect sizes, the choice of components must be made carefully. Especially, sample size calculations for composite binary endpoints depend not only on the anticipated effect sizes and event probabilities of the composite components, but also on the correlation between them. However, information on the correlation between endpoints is usually not reported in the literature which can be an obstacle for planning of future sound trial design. 
We consider two-arm randomized controlled trials with a primary composite binary endpoint and an endpoint that consists only of the clinically more important component of the composite endpoint. We propose a trial design that allows an adaptive modification of the primary endpoint based on blinded information obtained at an interim analysis. Especially, we consider a decision rule to select between a composite endpoint and its most relevant component as primary endpoint. The decision rule chooses the endpoint with the lower estimated required sample size. Additionally, the sample size is reassessed using the estimated event probabilities and correlation, and the expected effect sizes of the composite components. We investigate the statistical power and significance level under the proposed design through simulations. We show that the adaptive design is equally or more powerful than designs without adaptive modification on the primary endpoint. Besides, the targeted power is achieved even if the correlation is misspecified at the planning stage while maintaining the type 1 error. All the computations are implemented in R and illustrated by means of a peritoneal dialysis trial.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Platform trials and the future of evaluating therapeutic behavioural interventions</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bp_2022/</link>
      <pubDate>Tue, 04 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bp_2022/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Therapeutic interventions are typically evaluated in individual, parallel group trials, which are time consuming and provide limited information on comparative efficacy. Clinical psychology should leverage advances in other fields to improve and accelerate the evaluation process by adopting more efficient platform trials.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>On model-based time trend adjustments in platform trials with non-concurrent controls</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bp_2021/</link>
      <pubDate>Sat, 04 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bp_2021/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Platform trials can evaluate the efficacy of several treatments compared to a control. The number of treatments is not fixed, as arms may be added or removed as the trial progresses. Platform trials are more efficient than independent parallel-group trials because of using shared control groups. For arms entering the trial later, not all patients in the control group are randomised concurrently. The control group is then divided into concurrent and non-concurrent controls. Using non-concurrent controls (NCC) can improve the trial&amp;rsquo;s efficiency, but can introduce bias due to time trends.
We focus on a platform trial with two treatment arms and a common control arm. Assuming that the second treatment arm is added later, we assess the robustness of model-based approaches to adjust for time trends when using NCC. We consider approaches where time trends are modeled as linear or as a step function, with steps at times where arms enter or leave the trial. For trials with continuous or binary outcomes, we investigate the type 1 error (t1e) rate and power of testing the efficacy of the newly added arm under a range of scenarios. In addition to scenarios where time trends are equal across arms, we investigate settings with trends that are different or not additive in the model scale.
A step function model fitted on data from all arms gives increased power while controlling the t1e, as long as the time trends are equal for the different arms and additive on the model scale. This holds even if the trend&amp;rsquo;s shape deviates from a step function if block randomisation is used. But if trends differ between arms or are not additive on the model scale, t1e control may be lost.
The efficiency gained by using step function models to incorporate NCC can outweigh potential biases. However, the specifics of the trial, plausibility of different time trends, and robustness of results should be considered.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A class of two-sample nonparametric statistics for binary and time-to-event outcomes</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bg_2020/</link>
      <pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bg_2020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; We propose a class of two-sample statistics for testing the equality of proportions and the equality of survival functions. We build our proposal on a weighted combination of a score test for the difference in proportions and a Weighted Kaplan-Meier statistic-based test for the difference of survival functions. The proposed statistics are fully non-parametric and do not rely on the proportional hazards assumption for the survival outcome. We present the asymptotic distribution of these statistics, propose a variance estimator and show their asymptotic properties under fixed and local alternatives. We discuss different choices of weights including those that control the relative relevance of each outcome and emphasize the type of difference to be detected in the survival outcome. We evaluate the performance of these statistics with a simulation study, and illustrate their use with a randomized phase III cancer vaccine trial. We have implemented the proposed statistics in the R package SurvBin, available on GitHub (&lt;a href=&#34;https://github.com/MartaBofillRoig/SurvBin%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/MartaBofillRoig/SurvBin)&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Design of phase III trials with long-term survival outcomes based on short-term binary results</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bsg_2020/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bsg_2020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; 
Pathologic complete response (pCR) is a common primary endpoint for a phase II trial or even accelerated approval of neoadjuvant cancer therapy. If granted, a two-arm confirmatory trial is often required to demonstrate the efficacy with a time-to-event outcome such as overall survival. However, the design of a subsequent phase III trial based on prior information on the pCR effect is not straightforward. Aiming at designing such phase III trials with overall survival as primary endpoint using pCR information from previous trials, we consider a mixture model that incorporates both the survival and the binary endpoints. We propose to base the comparison between arms on the difference of the restricted mean survival times, and show how the effect size and sample size for overall survival rely on the probability of the binary response and the survival distribution by response status, both for each treatment arm. Moreover, we provide the sample size calculation under different scenarios and accompany them with an R package where all the computations have been implemented. We evaluate our proposal with a simulation study, and illustrate its application through a neoadjuvant breast cancer trial.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decision tool and Sample Size Calculator for Composite Endpoints</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bcg_2020/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bcg_2020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Summary points:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;This article considers the combination of two binary or two time-to-event endpoints to form the primary composite endpoint for leading a trial.&lt;/li&gt;
&lt;li&gt;It discusses the relative efficiency of choosing a composite endpoint over one of its components in terms of: the frequencies of observing each component; the relative treatment effect of the tested therapy; and the association between both components.&lt;/li&gt;
&lt;li&gt;We highlight the very important role of the association between components in choosing the most efficient endpoint to use as primary.&lt;/li&gt;
&lt;li&gt;For better grounded future trials, we recommend trialists to always reporting the association between components of the composite endpoint.&lt;/li&gt;
&lt;li&gt;Common fallacies to note when using composite endpoints: i) composite endpoints always imply higher power; ii) treatment effect on the composite endpoint is similar to the average effects of its components; and iii) the probability of observing the primary endpoint increases significantly.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>A new approach for sizing trials with composite binary endpoints using anticipated marginal values and accounting for the correlation between components</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bg_2018/</link>
      <pubDate>Tue, 03 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bg_2018/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; Composite binary endpoints are increasingly used as primary endpoints in clinical trials. When designing a trial, it is crucial to determine the appropriate sample size for testing the statistical differences between treatment groups for the primary endpoint. As shown in this work, when using a composite binary endpoint to size a trial, one needs to specify the event rates and the effect sizes of the composite components as well as the correlation between them. In practice, the marginal parameters of the components can be obtained from previous studies or pilot trials, however, the correlation is often not previously reported and thus usually unknown. We first show that the sample size for composite binary endpoints is strongly dependent on the correlation and, second, that slight deviations in the prior information on the marginal parameters may result in underpowered trials for achieving the study objectives at a pre-specified significance level. We propose a general strategy for calculating the required sample size when the correlation is not specified, and accounting for uncertainty in the marginal parameter values. We present the web platform CompARE to characterize composite endpoints and to calculate the sample size just as we propose in this paper. We evaluate the performance of the proposal with a simulation study, and illustrate it by means of a real case study using CompARE.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Selection of composite binary endpoints in clinical trials</title>
      <link>https://academic-demo.netlify.app/publication/journal-article/bg_2017/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/publication/journal-article/bg_2017/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Abstract:&lt;/strong&gt; The choice of a primary endpoint is an important issue when designing a clinical trial. It is common to use composite endpoints as a primary endpoint because it increases the number of observed events, captures more information and is expected to increase the power. However, combining events that have no similar clinical importance and have different treatment effects makes the interpretation of the results cumbersome and might reduce the power of the corresponding tests. Gómez and Lagakos proposed the ARE (asymptotic relative efficiency) method to choose between a composite or one of its components as primary endpoint comparing the efficacy of a treatment based on the times to each of these endpoints. The aim of this paper is to expand the ARE method to binary endpoints. We show that the ARE method depends on six parameters including the degree of association between components, event proportion, and effect of therapy given by the corresponding odds ratio of the single endpoints. A case study is presented to illustrate the methodology. We conclude with efficient guidelines for discerning which could be the best suited primary endpoint given anticipated parameters.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
